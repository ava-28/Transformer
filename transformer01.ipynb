{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ava-28/Transformer/blob/main/transformer01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teu69NymkSN8",
        "outputId": "3519a460-86bf-4bbb-8190-1c6fc370720f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: GPUtil in /usr/local/lib/python3.12/dist-packages (1.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install GPUtil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ayuZMs_vUpH",
        "outputId": "25f8b914-a0cf-4671-b242-236fac57fafa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TransformerNo1'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ava-28/TransformerNo1.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_5tks750N6U",
        "outputId": "19911a79-b034-4e19-fb13-bd65a15d0a9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'YOUR_REPO'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/YOUR_USERNAME/YOUR_REPO.git\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33d22600"
      },
      "source": [
        "### Scaled Dot-Product Attention\n",
        "\n",
        "This is the core of the attention mechanism. It takes three inputs: Query (Q), Key (K), and Value (V). It computes the dot product of the query with all keys, divides by the square root of the dimension of the keys ($√ d_k$), and then applies a softmax function to get the attention weights. These weights are then multiplied by the values to get the output.\n",
        "\n",
        "The formula is:\n",
        "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FtuxHjR-CLv",
        "outputId": "b39d0254-1117-49e5-dbae-908a92070434"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torchtext 0.18.0\n",
            "Uninstalling torchtext-0.18.0:\n",
            "  Successfully uninstalled torchtext-0.18.0\n",
            "Found existing installation: torchdata 0.7.1\n",
            "Uninstalling torchdata-0.7.1:\n",
            "  Successfully uninstalled torchdata-0.7.1\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/main.py\", line 78, in main\n",
            "    command = create_command(cmd_name, isolated=(\"--isolated\" in cmd_args))\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/__init__.py\", line 114, in create_command\n",
            "    module = importlib.import_module(module_path)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/install.py\", line 15, in <module>\n",
            "    from pip._internal.cli.req_command import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/req_command.py\", line 18, in <module>\n",
            "    from pip._internal.index.collector import LinkCollector\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/index/collector.py\", line 31, in <module>\n",
            "    from pip._vendor import requests\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/requests/__init__.py\", line 159, in <module>\n",
            "    from .api import delete, get, head, options, patch, post, put, request\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/requests/api.py\", line 11, in <module>\n",
            "    from . import sessions\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/requests/sessions.py\", line 15, in <module>\n",
            "    from .adapters import HTTPAdapter\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/requests/adapters.py\", line 81, in <module>\n",
            "    _preloaded_ssl_context.load_verify_locations(\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Collecting de-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.8.0/de_core_news_sm-3.8.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# Uninstall existing torchtext and torchdata first to ensure a clean install\n",
        "!pip uninstall -y torchtext torchdata\n",
        "# Then install the specified versions along with others\n",
        "!pip install -q torchdata==0.7.1 torchtext==0.18.0 spacy==3.8.0 altair GPUtil\n",
        "!python -m spacy download de_core_news_sm\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "VhoxD8OBpx-L",
        "outputId": "65e050e2-a701-4fb7-fbc3-ce194ae9b27b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torchtext'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2194114085.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maltair\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_map_style_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_vocab_from_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "\n",
        "\n",
        "import os\n",
        "from os.path import exists\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import log_softmax, pad\n",
        "import math\n",
        "import copy\n",
        "import time\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import torchtext.datasets as datasets\n",
        "import spacy\n",
        "import GPUtil\n",
        "import warnings\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "RUN_EXAMPLES = True"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "740547d5",
        "outputId": "b00968df-bff6-4a36-bbed-fb14f64cfa6a"
      },
      "source": [
        "import torchtext\n",
        "print(f\"torchtext version: {torchtext.__version__}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torchtext'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3945171380.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"torchtext version: {torchtext.__version__}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1zUGHgYp0hf"
      },
      "outputs": [],
      "source": [
        "# Some convenience helper functions used throughout the notebook\n",
        "\n",
        "\n",
        "def is_interactive_notebook():\n",
        "    return __name__ == \"__main__\"\n",
        "\n",
        "\n",
        "def show_example(fn, args=[]):\n",
        "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
        "        return fn(*args)\n",
        "\n",
        "\n",
        "def execute_example(fn, args=[]):\n",
        "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
        "        fn(*args)\n",
        "\n",
        "\n",
        "class DummyOptimizer(torch.optim.Optimizer):\n",
        "    def __init__(self):\n",
        "        self.param_groups = [{\"lr\": 0}]\n",
        "        None\n",
        "\n",
        "    def step(self):\n",
        "        None\n",
        "\n",
        "    def zero_grad(self, set_to_none=False):\n",
        "        None\n",
        "\n",
        "\n",
        "class DummyScheduler:\n",
        "    def step(self):\n",
        "        None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEf1iyzxat_U"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard Encoder-Decoder architecture. Base for this and many\n",
        "    other models.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.generator = generator\n",
        "\n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        \"Take in and process masked src and target sequences.\"\n",
        "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        return self.encoder(self.src_embed(src), src_mask)\n",
        "\n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4Z-xAy9mJi5"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    \"Define standard linear + softmax generation step.\"\n",
        "\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return log_softmax(self.proj(x), dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFjcn-bIqfK4"
      },
      "outputs": [],
      "source": [
        "def clones(module, N):\n",
        "  \"produce N identical layers\"\n",
        "  return nn.ModuleList([copy.deepcopy(module) for _ in range (N)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05UAw0aUukma"
      },
      "outputs": [],
      "source": [
        "from torch.nn.modules.normalization import LayerNorm\n",
        "class Encoder(nn.Module):\n",
        "  \"Core ecnoder is a stack of N layers\"\n",
        "  def __init__(self, layer, N):\n",
        "    super(Encoder,self).__init__()\n",
        "    self.layers = clones(layer, N)\n",
        "    self.norm = LayerNorm(layer.size)\n",
        "\n",
        "def forward(self, x, mask):\n",
        "  \"pass the input (and mask) throuhg each layer in turn\"\n",
        "  for layer in self.layers:\n",
        "    x = layer(x, mask)\n",
        "  return self.norm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlwhRJdt1SOA"
      },
      "outputs": [],
      "source": [
        "\"defining the LayerNorm class\"\n",
        "class LayerNorm(nn.Module):\n",
        "  \"Construct a layernorm module\"\n",
        "  def __init__ (self, features, eps = 1e-6):\n",
        "    super(LayerNorm, self). __init__()\n",
        "    self.a_2 = nn.Parameter(torch.ones(features))\n",
        "    self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "    self.eps = eps\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(-1, keepim =True)\n",
        "    std = x.std(-1, keepdim = True )\n",
        "    return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0II6lijV4zjT"
      },
      "outputs": [],
      "source": [
        "class SublayerConnection (nn.Module):\n",
        "  \"\"\"\n",
        "  A residual connertion followed by a layer norm.\n",
        "  \"\"\"\n",
        "  def __init__ (self,size, dropout):\n",
        "    super(SublayerConnection, self).__init__()\n",
        "    self.norm = LayerNorm(size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, sublayer):\n",
        "    \"Apply residual connection to any sublayer with the same size\"\n",
        "    return x + self.dropout(sublayer(self.norm(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCsnUuTO07Zy"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer (nn.Module):\n",
        "  \"Encoder is made up from self-attention and feed forward\"\n",
        "  def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    self.self_attn = self_attn\n",
        "    seld.feed_forward = feed_forward\n",
        "    self.sublayer = clones(SublayerConnection(size,dropout), 2)\n",
        "    self.size = size\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    #So:  self.sublayer[0] → Add & Norm wrapper for self-attention.   self.sublayer[1] → Add & Norm wrapper for FFN#\n",
        "    #self.self_attn(x, x, x, mask) means: Query = Key = Value = x.      ---> Tokens attend to each other#\n",
        "\n",
        "    x = self.sublayer[0](\n",
        "        x,\n",
        "        lambda x: self,self_attn(x,x,x, mask)\n",
        "    )\n",
        "    return self.sublayer [1] (x, self.feed_forward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bar92YIMvtPt"
      },
      "source": [
        "Here we start with the Decoder:\n",
        "\n",
        "\n",
        "the decoder is cpomposed of a stack pf N = 6 identical layers. So we use the same N here as well.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkJWlKhCML2h"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  \"Generic N layer decoder with masking\"\n",
        "\n",
        "    def __init__(self, layer, N):\n",
        "      super(Decoder, self).__init__()\n",
        "      self.layers = clones(layer, N)\n",
        "      self.norm = LayerNorm (layer.size)\n",
        "\n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "      for layer in self.layers:\n",
        "        x = layer (x, memory, src_mask, tgt_mask)\n",
        "      return self.norm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YBDeP-F20RO"
      },
      "source": [
        "we have three parts here:\n",
        "1. self attention (over the output of the the decoder)\n",
        "2. sequence attention (over the output of both encoder and the decoder0\n",
        "3. Feed forward\n",
        "\n",
        "we have to also employ the residual connections around each of the sub-layers of the decoder, followed by layer normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNt3HsWUz7CL"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  \"Decoder is made of self-attn, and feed froward. we have the size for it, the initial input of the decoder via self attention(what it had produced before)\"\n",
        "  \"the multi haed attention for the encoder decoder layer, the feed forwrd, and the dropout\"\n",
        "\n",
        "  def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "    super(DecoderLayer, self).__inti__()\n",
        "    self.size = size\n",
        "    self.self_attn = self_attn\n",
        "    self.src_attn = self_attn\n",
        "    self.feed_forward = feed_forward\n",
        "    self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        "\n",
        "\n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "\n",
        "      m = memory\n",
        "      x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "      x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "      return self.sublayer[2](x, self.feed_forward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oW-8JmhV-_NV"
      },
      "outputs": [],
      "source": [
        "def subsequent_mask (size):\n",
        "  \"Mask out subsequent positions.\"\n",
        "  attn_shape = (1, size, size)\n",
        "  subsequence_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
        "      torch.uint8\n",
        "      # torch.uint8 = Casts the tensor to an older boolean-ish integer type (0/1)#\n",
        "  )\n",
        "  return subsequent_mask == 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPTmtBvuxS7T"
      },
      "outputs": [],
      "source": [
        "RUN_EXAMPLES = True\n",
        "# Some convenience helper functions used throughout the notebook\n",
        "\n",
        "\n",
        "def is_interactive_notebook():\n",
        "    return __name__ == \"__main__\"\n",
        "\n",
        "\n",
        "def show_example(fn, args=[]):\n",
        "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
        "        return fn(*args)\n",
        "\n",
        "\n",
        "def execute_example(fn, args=[]):\n",
        "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
        "        fn(*args)\n",
        "\n",
        "\n",
        "class DummyOptimizer(torch.optim.Optimizer):\n",
        "    def __init__(self):\n",
        "        self.param_groups = [{\"lr\": 0}]\n",
        "        None\n",
        "\n",
        "    def step(self):\n",
        "        None\n",
        "\n",
        "    def zero_grad(self, set_to_none=False):\n",
        "        None\n",
        "\n",
        "\n",
        "class DummyScheduler:\n",
        "    def step(self):\n",
        "        None\n",
        "\n",
        "\n",
        "def example_mask():\n",
        "    LS_data = pd.concat(\n",
        "        [\n",
        "            pd.DataFrame(\n",
        "                {\n",
        "                    \"Subsequent Mask\": subsequent_mask(20)[0][x, y].flatten(),\n",
        "                    \"Window\": y,\n",
        "                    \"Masking\": x,\n",
        "                }\n",
        "            )\n",
        "            for y in range(20)\n",
        "            for x in range(20)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return (\n",
        "        alt.Chart(LS_data)\n",
        "        .mark_rect()\n",
        "        .properties(height=250, width=250)\n",
        "        .encode(\n",
        "            alt.X(\"Window:O\"),\n",
        "            alt.Y(\"Masking:O\"),\n",
        "            alt.Color(\"Subsequent Mask:Q\", scale=alt.Scale(scheme=\"viridis\")),\n",
        "        )\n",
        "        .interactive()\n",
        "    )\n",
        "\n",
        "\n",
        "show_example(example_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " The code is showing the position each tagget word (row) is allowed to look at - words cant see positions father thn themselves.\n",
        "\n",
        "\n",
        " **Attention**\n",
        " now here we are going to define the attention structure.\n"
      ],
      "metadata": {
        "id": "-pV6lJTbH9YL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attention(query, key, value, mask = None, droput = None)\n",
        "  \"compute Scaled dot product of of Q and K\"\n",
        "  d_k = query.size(-1) \"the query is tensor of shape (batch_size, sequence_length, features). then .size(-1) will capture the last dimention - feature\"\n",
        "  scores = torch.matmul(query, key.transpose(-2, -1))/ math.sqrt(d_k)\n",
        "  if mask is not None:\n",
        "    scores = scores.masked_fill(mask == 0, -1e9) \"mask == 0 is a boolean mask where ture is when the position should be ignored\"\n",
        "    \".masked_filled is going to fill any true position with the value -1e9\"\n",
        "  p_attn = scores.softmax(dim = -1) \"gets appied to the attention scores of softmax along the last layer.\n",
        "\n",
        "  if dropout is not None:\n",
        "    p_attn = dropout(p_attn)\n",
        "\n",
        "  return troch.matmul(p_attn, value), p_attn"
      ],
      "metadata": {
        "id": "hbsRypEyJRjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MULTI HEAD ATTENTION**\n",
        "concates the heads where the heads are made from the attention layers: Attention (QW,KW,VW)\n",
        "\n",
        "d_k = d_v = d_model/h = 64"
      ],
      "metadata": {
        "id": "tk3lHgf1L0iW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadedAttention(nn.Module)\n",
        "def __init__(self, h, d_model, dropout=0.1):\n",
        "  \"Take in the model size and numbers of heads.\"\n",
        "\n",
        "  super(MultiHeadedAttention, self).__init__()\n",
        "  assert d_model % h == 0 \"assert is used to double check if: 1. d_model is divisible by the number of heads as each layer of heads will process some part of the data\"\n",
        "  self.d_k = d_model // h \"2. the division must give out an integer.\"\n",
        "  self.h = h\n",
        "  self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "  self.attn = None\n",
        "  self.dropout = nn.Dropout(p = dropout)\n",
        "\n",
        "\n",
        "\n",
        "def forward(self, query, key, value, mask=None):\n",
        "  \"now we are at feed forward\"\n",
        "  if mask is not None:\n",
        "    # apply some mask on the layers of h\n",
        "    mask = mask.unsqueeze (1) #is used to add a new dimension of size 1 to a tensor at a specified position\n",
        "  nbatches = query.size (0) # query = (batch_size, sequence_length, embedding_dimension)\n",
        "\n",
        "  # 1) we start from doing all the linear projections in batch from d_model => h\n",
        "# x d_k\n",
        "query,key, value = [\n",
        "    lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1 ,2)\n",
        "    for lin, x in zip(self.linears, (query, key, value)) #this function pairs up elements\n",
        "]\n",
        "\n",
        "# 2) apply attention on all project vectors in the batch.\n",
        "\n",
        "x,  self.attn = attention(\n",
        "    query, key, value, mask=mask, dropout=self.dropout\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "# 3) \"Concat\" using a view and apply a final linear\n",
        "\n",
        "x = (\n",
        "    x.transpose(1, 2)\n",
        "    .contiguous() # is related to how tensors are stored in memory\n",
        "    .view(nbatches, -1, self.h * self.d_k)\n",
        ")\n",
        "\n",
        "del query\n",
        "del key\n",
        "del value\n",
        "return self.linears [-1](x)"
      ],
      "metadata": {
        "id": "r2qWge0jLb0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Position-wise Feed-Forward Networks\n",
        "\n",
        "FFN(x) = max(0, xW1 + b1)W2 + b2\n"
      ],
      "metadata": {
        "id": "dGZqAjfwlrTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "  def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "    super(PositionwiseFeedForward, self).__init__()\n",
        "    self.w_1 = nn.Linear(d_model, d_ff) #the biases for each of these terms are included in the nn.Linear\n",
        "    self.w_2 = nn.Linear(d_ff, d_model) #the biases for each of these terms are included in the nn.Linear\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x)\n",
        "  return self.w_2(self.dropout(self.w_1(x).relu()))"
      ],
      "metadata": {
        "id": "H8wFeWljlrA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OKahhslOpYja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EMBEDDINGS AND SOFTMAX**\n",
        "\n",
        "convert the input tokens and output tokens to vctor dimension d_model"
      ],
      "metadata": {
        "id": "BrdSlDV4o5F9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "  def __init__(self, d_model, vocab):\n",
        "    super(Embeddings, self).__init__()\n",
        "    self.lut = nn.Embedding(vocab, d_model)\n",
        "    self.d_model = d_model\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.lut(x) * math.sqrt(self.d_model)"
      ],
      "metadata": {
        "id": "7Gjh6YggpZDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POSITIONAL ENCODING**\n",
        "\n",
        "usibg the frequenceis, we find the position of each token. in here the position is pos, i is the dimension. each dimension of the positional encoding corresponds to a sinusoid."
      ],
      "metadata": {
        "id": "BJxmHYJqsOF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  \"implementing the PE function.\"\n",
        "\n",
        "  def __init__(self, d_model, dropout, max_len=5000) #the max sequenc length the model can handel\n",
        "  super(PositionalEncoding, self).__init__()\n",
        "  sel.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "  #compute the positional encodings once in log space\n",
        "\n",
        "  pe = torch.zeros(max_len, d_model) #torch.zeros = tensor filled with the scalar 0\n",
        "  position = torch.arange(0, max_len).unsqueeze(1) #This function creates a 1D tensor with values from 0 up to (but not including) max_len\n",
        "  div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "\n",
        "  pe[:, 0::2] = torch.sin(position * div_term)\n",
        "  pe[:, 1::2] = torch.cos(position * div_term)\n",
        "  pe = pe.unsqueeze(0)"
      ],
      "metadata": {
        "id": "9tJN1HhitHxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4193b5e7"
      },
      "source": [
        "# Task\n",
        "Please manually restart the Colab runtime (Runtime -> Restart runtime) to apply the package installation changes. After the runtime has restarted, please approve to continue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c19849d7"
      },
      "source": [
        "## User Restart Runtime\n",
        "\n",
        "### Subtask:\n",
        "Manually restart the Colab runtime to apply the package installation changes and ensure all dependencies are loaded correctly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7e7969a"
      },
      "source": [
        "#### Instructions\n",
        "1. Go to the 'Runtime' menu at the top of the Colab notebook.\n",
        "2. Select 'Restart runtime' from the dropdown menu.\n",
        "3. Confirm the restart when prompted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "690bda9c"
      },
      "source": [
        "## Execute Modified Cells\n",
        "\n",
        "### Subtask:\n",
        "After the runtime restart, execute cell `6FtuxHjR-CLv` and then cell `LPTmtBvuxS7T` to ensure the correct versions of `torchdata` and `torchtext` are installed and `RUN_EXAMPLES` is defined. This step also implicitly handles `torchtext` import, and if it still fails, further investigation might be needed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baf1288b"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step in the subtask is to execute cell `6FtuxHjR-CLv` to install necessary packages and ensure correct versions of `torchdata` and `torchtext`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80b66aac"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# Uninstall existing torchtext and torchdata first to ensure a clean install\n",
        "!pip uninstall -y torchtext torchdata\n",
        "# Then install the specified versions along with others\n",
        "!pip install -q torchdata==0.7.1 torchtext==0.18.0 spacy==3.8.0 altair GPUtil\n",
        "!python -m spacy download de_core_news_sm\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e26fdb5"
      },
      "source": [
        "**Reasoning**:\n",
        "After executing cell `6FtuxHjR-CLv` to install the correct package versions, the next logical step is to execute cell `VhoxD8OBpx-L` which defines necessary imports and global variables like `RUN_EXAMPLES`. This is crucial for `LPTmtBvuxS7T` to execute without a `NameError`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37d3827e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from os.path import exists\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import log_softmax, pad\n",
        "import math\n",
        "import copy\n",
        "import time\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import torchtext.datasets as datasets\n",
        "import spacy\n",
        "import GPUtil\n",
        "import warnings\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "RUN_EXAMPLES = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ff2d99b"
      },
      "source": [
        "**Reasoning**:\n",
        "The OSError when importing `torchtext` is likely due to an ABI incompatibility caused by the `torchdata` version conflict reported during package installation. I will modify the installation command to install `torchdata==0.11.0` as required by `torchtune`, hoping it resolves the compatibility issue for `torchtext`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6b7fd493"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "!pip uninstall -y torchtext torchdata\n",
        "!pip install -q torchdata==0.11.0 torchtext==0.18.0 spacy==3.8.0 altair GPUtil\n",
        "!python -m spacy download de_core_news_sm\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcca8411"
      },
      "source": [
        "## User Restart Runtime\n",
        "\n",
        "### Subtask:\n",
        "Manually restart the Colab runtime to apply the package installation changes and ensure all dependencies are loaded correctly.\n",
        "\n",
        "#### Instructions\n",
        "1. Go to the 'Runtime' menu at the top of the Colab notebook.\n",
        "2. Select 'Restart runtime' from the dropdown menu.\n",
        "3. Confirm the restart when prompted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7955660a"
      },
      "source": [
        "**Reasoning**:\n",
        "The runtime is assumed to have been restarted. According to the updated subtask, the next step is to re-execute cell `6FtuxHjR-CLv` to ensure correct package versions are installed after the restart.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ab9d980"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# Uninstall existing torchtext and torchdata first to ensure a clean install\n",
        "!pip uninstall -y torchtext torchdata\n",
        "# Then install the specified versions along with others\n",
        "!pip install -q torchdata==0.11.0 torchtext==0.18.0 spacy==3.8.0 altair GPUtil\n",
        "!python -m spacy download de_core_news_sm\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7262b552"
      },
      "source": [
        "**Reasoning**:\n",
        "After successfully re-executing cell `6FtuxHjR-CLv` (which installs packages) and assuming a runtime restart, the next crucial step is to execute cell `VhoxD8OBpx-L`. This cell defines essential imports, including `torchtext` which previously caused an `OSError`, and also defines the global variable `RUN_EXAMPLES`. Executing this cell is a prerequisite for `LPTmtBvuxS7T` to run without a `NameError` and to check if the `torchtext` import issue is finally resolved.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ded6d35"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from os.path import exists\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import log_softmax, pad\n",
        "import math\n",
        "import copy\n",
        "import time\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import torchtext.datasets as datasets\n",
        "import spacy\n",
        "import GPUtil\n",
        "import warnings\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "RUN_EXAMPLES = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91d4937b"
      },
      "source": [
        "**Reasoning**:\n",
        "The persistent `OSError` with `torchtext` despite reinstallations and runtime restarts indicates an ABI incompatibility between `torchtext==0.18.0` and the underlying PyTorch installation in the Colab environment. Downgrading `torchtext` to a slightly older version, `0.17.0`, which is often more compatible with various PyTorch versions, might resolve this symbol loading issue. The `torchdata==0.11.0` will be kept to maintain `torchtune` compatibility.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6350f4f8"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "!pip uninstall -y torchtext torchdata\n",
        "!pip install -q torchdata==0.11.0 torchtext==0.17.0 spacy==3.8.0 altair GPUtil\n",
        "!python -m spacy download de_core_news_sm\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de2d9588"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to install `torchtext==0.17.0` failed as it was not found. Consulting the available versions from the error message, `torchtext==0.17.2` is a viable older version. This version will be installed, keeping `torchdata==0.11.0`, to try and resolve the persistent `OSError` due to ABI incompatibility with the underlying PyTorch installation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "797e6aa7"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "!pip uninstall -y numpy pandas torchtext torchdata\n",
        "!pip install -q numpy pandas torchdata==0.11.0 torchtext==0.17.2 spacy==3.8.0 altair GPUtil\n",
        "!python -m spacy download de_core_news_sm\n",
        "!python -m spacy download en_core_web_sm"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMU052omUVUq2rp/cOsCkui",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}